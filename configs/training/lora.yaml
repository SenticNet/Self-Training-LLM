lora_r: 32
lora_alpha: 32
lora_dropout: 0.05
quantized: True
lora_layers: ['down_proj', 'up_proj', 'gate_proj', 'lm_head', 'q_proj', 'k_proj', 'v_proj', 'o_proj']
