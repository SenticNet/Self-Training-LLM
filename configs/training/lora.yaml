lora_r: 32
lora_alpha: 32
lora_dropout: 0.05
quantized: True
lora_layers: ['down_proj', 'up_proj', 'gate_proj', 'q_proj', 'k_proj', 'v_proj', 'o_proj']
learning_rate: 5.0e-6
optim: paged_adamw_32bit